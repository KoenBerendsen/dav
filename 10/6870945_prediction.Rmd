---
title: "Assignment: Prediction"
author: "David Vichansky"
date: "22-01-2021"
mainfont: Garamond
fontsize: 12pt
urlcolor: blue
output: 
  pdf_document:
    latex_engine: xelatex
---

Load the packages.
```{r load_packages}
library(igraph)
library(ggdendro)
library(dendextend)
library(ISLR)
library(tidyverse)
library(haven)
library(MASS)
library(class)
library(rpart)
library(rpart.plot)
library(pROC)
library(randomForest)
library(tree)
library(glmnet)
```

## Description of the Data set

For this assignment we have used a dataset of UK Traffic Accidents found on kaggle. (https://www.kaggle.com/daveianhickey/2000-16-traffic-flow-england-scotland-wales/data?select=accidents_2012_to_2014.csv). Given the size of the dataset  (over 600 MB)  we decided to focus only on the timeframe 2012 to 2014. This still gives us over 450.000 data entries to work with. The data is very well structured and requires no dropping of data due to incomplete records. 
The data set contains information coming from police reports on major car accidents throughout the UK. In all there are 33 columns containing location information (coordinates, police force and administrative area), date information (date, time, day of the week, year), information on the road where the accident took place (weather, type of road, overpasses, junctions) and the severity of the accident (severity, casualties).   Further descriptions for the columns can be found in the file 7752_road-accident-safety-data-guide.xls.
Given this breadth of ordinal, categorical and numerical information, there are many different questions that can be looked into.  For example, one could attempt to predict the accident severity based on the type of street and weather conditions. Given the large amount of data it should also be possible to split the data set into many individual sets and tune the parameters of the given predictions. 





1. 

```{r 1}
## Load data set
accident <- read.csv("data/accidents_2012_to_2014.csv", header = TRUE)
```

2.
```{r 2}
## See data sets column names
colnames(accident)
```

3.
```{r 3}
## Filter mostly for numeric columns or ones that help with classification or prediction, i.e. 'police_force' is most likely an independent variable from accidents occuring
accident <- as_tibble(accident)

myvars <- names(accident) %in% c("Accident_Severity", 
                                 "Number_of_Vehicles", 
                                 "Number_of_Casualties",
                                 "Day_of_Week",
                                 #"Road_Type",
                                 "Speed_limit",
                                 #"Light_Conditions",
                                 #"Weather_Conditions",
                                 #"Road_Surface_Conditions",
                                 "Urban_or_Rural_Area",
                                 "Year")

accident_new <- accident[myvars]
```

4.
```{r 4}
## Create 'test' and 'train' data sets
#Split data set into training and test
training_size <- floor(0.70 * nrow(accident_new))

training_ind <- sample(seq_len(nrow(accident_new)), size = training_size, replace = FALSE)

#Train data set
accident_training <- accident_new[training_ind, ]

#Test data set
accident_test <- accident_new[-training_ind, ]
```

5.
```{r 6}
## Check data type
sapply(accident_new, class)
```

6.
```{r 6}
# Fit lda model, i.e. calculate model parameters, using 'integer' variables only, we will use the 'factor' variables to map plots later
accident_lda <- lda(Number_of_Vehicles ~ Accident_Severity + Number_of_Casualties + Speed_limit + Urban_or_Rural_Area + Year, data = accident_training)

accident_lda
```

7.
```{r 7}
## Create a confusion matrix and assess model performance on the 'test' data set

## Use test data set
accident_pred <- predict(accident_lda, accident_test)

## Now use the 'class' feature to assess performance
lda_class <- accident_pred$class


## Calculate the accuracy (diagonal entries) for this table
conf_mat <- table(true = accident_test$Number_of_Vehicles, predicted = lda_class)

lda_acc <- (sum(conf_mat[1,1] + conf_mat[2,2]) / sum(conf_mat))

## Print 'accuracy' result (sum of diagonal entires/sum of all entries)
paste(round(lda_acc*100, 2), "%", sep="")
```

We therefore observe a roughly 60% accuracy rate.

8.
```{r 8}
## Create a classification tree using rpart() for the variable 'Number_of_Vehicles'

##Use all the variables from the complete complete data set 'accident_new', as oppose to using the train/test split

## Play around with using 'minsplit': “the minimum number of observations that must exist in a node in order for a split to be attempted” and 'minbucket': “the minimum number of observations in any terminal node”

## Convert data set back to data frame
accident_df <- data.frame(accident_new)

accident_tree <- rpart(Number_of_Vehicles ~ . , data = accident_df, minsplit=4)

##Accident_Severity + Number_of_Casualties + Speed_limit + Urban_or_Rural_Area + Year

rpart.plot(accident_tree)
```


```{r 9}
## LASSO regression

## Create an array of different lambda functions ranging from 10^-1 and 10^2
grid <- 10^seq(2,-1, length =100)

## Remove the 'predictor' variable
#x_train <- accident_training[, -2]

#x_train <- model.matrix(Number_of_Vehicles ~ Accident_Severity + Number_of_Casualties + Speed_limit + Urban_or_Rural_Area + Year, data = accident_training)[, -2]

#y_train <- accident_training$Number_of_Vehicles

## Convert data frame to numeric
y_train_df <- sapply(accident_training[, 2], as.numeric)

accident_training_df <- sapply(accident_training[, -2], as.numeric)

## Determine best lambda to perfrom Lasso regression with
cv_out <- cv.glmnet(accident_training_df, y_train_df, alpha=1)

## Take 'best' lambda
lambda_min <- cv_out$lambda.min

## Perfrom Lasso regression
lasso_mod <- glmnet(accident_training_df, y_train_df, alpha=1, lambda=lambda_min)

## Predict using LASSO regression
lasso_coef <- predict(lasso_mod, type="coefficients", s=lambda_min)

lasso_coef
```

We observe that the variables with the largest 'impact' are 'Accident_Severity', 'Number_of_Casualties' and 'Urban_or_Rural_Area'. 

```{r 10}
## Perfrom a second linear discriminant analysis model using the formula: 'Accident_Severity + Number_of_Casualties + Urban_or_Rural_Area', creating a second confusion matrix and assessing difference in 'accuracy'

accident_lda2 <- lda(Number_of_Vehicles ~ Accident_Severity + Number_of_Casualties + Urban_or_Rural_Area, data = accident_training)


## Use test data set
accident_pred2 <- predict(accident_lda2, accident_test)

## Now use the 'class' feature to assess performance
lda_class2 <- accident_pred2$class


## Calculate the accuracy (diagonal entries) for this table
conf_mat2 <- table(true = accident_test$Number_of_Vehicles, predicted = lda_class2)

lda_acc2 <- (sum(conf_mat2[1,1] + conf_mat2[2,2]) / sum(conf_mat2))

## Print 'accuracy' result (sum of diagonal entires/sum of all entries)
paste(round(lda_acc2*100, 2), "%", sep="")
```

Sometimes there is improvement, but very marginal.



=================
Philipp's section


```{r }
## Filter mostly for numeric columns or ones that help with classification or prediction, i.e. 'police_force' is most likely an independent variable from accidents occuring

accident <- read.csv("accidents_2012_to_2014.csv", header = TRUE)
accident <- as_tibble(accident)

myvars <- names(accident) %in% c("Accident_Severity", 
                                 "Number_of_Vehicles", 
                                 "Number_of_Casualties",
                                 #"Date",
                                 #"Time",
                                 "Day_of_Week",
                                 "Road_Type",
                                 "Speed_limit",
                                 "Road_Surface_Conditions")

accident_phil <- accident[myvars]

```

```{r}
## Create 'test' and 'train' data sets
#Split data set into training and test
training_size <- floor(0.10 * nrow(accident_phil))

training_ind <- sample(seq_len(nrow(accident_phil)), size = training_size, replace = FALSE)

#Train data set
accident_training <- accident_phil[training_ind, ]

#Test data set
accident_test <- accident_phil[-training_ind, ]
```


```{r}
library(GGally)
ggpairs(accident_training, ggplot2::aes(colour=Road_Surface_Conditions)) 
```

```{r}
plot <- ggplot(aes(x = Speed_limit, y = Number_of_Casualties), data = accident_training) +
    geom_point() +
    #geom_line(aes(x = lstat, y = predict(model)), color = "red", size =1) +
    #ggtitle(paste(deparse(substitute(model)))) + # generates name of the input model as title
    theme_minimal()

plot
  
```
```{r}
ggplot(accident_new, aes(x=Number_of_Vehicles)) + geom_histogram()
```
```{r}
ggplot(accident_new, aes(x=Accident_Severity)) + geom_histogram()
```
```{r}
# Downsampling to make classes equally largge


sev_1 <- accident_new %>% 
  filter( Accident_Severity == 1)

sev_2 <- accident_new %>% 
  filter( Accident_Severity == 2)

sev_3 <- accident_new %>% 
  filter( Accident_Severity == 3)

dim(sev_1)
dim(sev_2)
dim(sev_3)

df_balanced <- rbind(sev_1, sample_n(sev_2, nrow(sev_1)), sample_n(sev_3, nrow(sev_1)))

```

```{r}
ggplot(df_balanced, aes(x=Accident_Severity)) + geom_histogram()
```

